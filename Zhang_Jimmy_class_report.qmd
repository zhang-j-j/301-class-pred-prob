---
title: "Classification Prediction Problem Report"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "Jimmy Zhang"
pagetitle: "Classification Report Jimmy Zhang"
date: today

format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    code-fold: false
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin 
---

::: {.callout-tip icon=false}

## Github Repo Link

[Jimmy's Classification Github Repo (zhang-j-j)](https://github.com/stat301-3-2025-spring/class-pred-prob-zhang-j-j)

:::

## Overview

The goal of this classification problem is to predict if an Airbnb listing is from a superhost.[^data] The assessment metric in this competition was area under the ROC curve (ROC_AUC), and a variety of predictive modeling techniques were implemented over multiple attempts to develop the best performing model. While the full results from the private leaderboard are not available, both of my submissions achieved the top benchmark on the public leaderboard (ROC_AUC greater than 0.975). Some of my other models that were not submitted also attained this level of performance.

[^data]: [Link to Kaggle dataset](https://www.kaggle.com/competitions/classification-spring-2025-airbnb-super-host/data) (downloaded April 2, 2025).

## Model Development

Some initial data cleaning was required to prepare the raw data for modeling. I then completed three iterations of the model development and refinement process. The first stage served as a high-level screening process to compare results between model types using minimal preprocessing. My next attempts were informed by these initial takeaways, and I also introduced more complex feature engineering steps in each. The second attempt considered the top-performing model types (random forest and boosted tree) more closely, and the third attempt refined the best models from attempt 2 while also trying ensemble models.

### Submission 1

My top-performing model on the public leaderboard was the 2nd best boosted tree (`lightgbm`) from the tuning stage of the second iteration. The hyperparameter values for this model are: 90 randomly selected predictors (`mtry`), 1000 trees (`trees`), minimum node size of 2 (`min_n`), and a learning rate of about 0.0611 (`learn_rate`). Because I directly used the entire competition dataset without saving a testing set for this attempt, I was unable to independently assess this model's performance. However, the estimate from resampling (ROC_AUC = 0.9774) was consistent with performance on the public leaderboard (within 1 standard error).

Beyond using all of the original (reasonable) predictors, I included additional feature engineering steps to capture general listing details (i.e. consistency in location, presence of host information, time between first and last reviews), specific amenities (i.e. pool, dishwasher), and other potential predictive relationships. I used grid search to tune hyperparameter values for each model type with resamples before refitting the best performing submodels to the entire training set and computing predictions for submission. Despite the inability to assess this model, I still do not expect much overfitting from this model to the public leaderboard. All of my decisions were based on directly analyzing the tuning results, and I did not consult the leaderboard until making my submissions.

### Submission 2

While my second attempt already reached the top benchmark, my third attempt sought to further improve upon those results. I split the competition dataset into training and testing sets for this attempt and introduced some more predictor variables through feature engineering. Notably, one major addition was a column for the number of listings each host has in the dataset (by counting repeated instances of host information). One consideration with that was how to handle the final prediction set, and my results indicated that performance was consistently better when counting instances within only the testing set (not combining counts from all of the data). Otherwise, I followed a similar overall procedure, and the only minor difference was an extra step to assess the submodels and ensembles before retraining them on the full competition set.

My selected model was the best-performing model on both the resamples and assessment set with an ROC_AUC value of 0.9777 (@fig-roc). This boosted tree (`lightgbm`) used the hyperparameters `mtry = 65`, `trees = 1200`, `min_n = 2`, `learn_rate = 0.06`, and maximum tree depth of 15 (`tree_depth`). Interestingly, this model performed considerably worse on the public leaderboard and did not even achieve the top benchmark. I suspect that this could be due to how I added the new listing counts column to the final testing set, but the alternative method (which I expected to be better, since it is more similar to how I handled the assessment set) was consistently worse on the public leaderboard across many submissions. Nonetheless, I still selected this as my second submission on account of its strong performance estimates from resampling and the assessment set.

![Plot the ROC curve from the testing set predictions.](figures/fig_roc.png){#fig-roc}

## Conclusion

While these two were selected as the final submissions, I produced a number of other submissions that achieved the top benchmark on the public leaderboard. Notably, these were almost all boosted tree models from my second attempt, suggesting that the additional feature engineering steps for the third attempt were not helpful. This was surprising, since I would expect that a host with more listings is more likely to be a superhost. However, my implementation was not fully robust (particularly to missing values), so it is possible that there were hidden issues with this predictor. I would begin any future work by examining this more closely, and other improvements would likely lie in additional feature engineering steps, though it might be difficult to make many further improvements considering the current level of performance. Overall, this was a very successful process that developed a strong predictive model.

## Comment on Generative AI

I used some help from generative AI (Microsoft Copilot and ChatGPT) for debugging errors in my code. I would typically try to figure out the issues on my own first (which was sufficient for most cases), but I would search online or ask AI if I was struggling to identify the issue. I also consulted AI to try and build a custom recipe step (counting the number of listings per host), but I ended up implementing this directly on the datasets instead of within the recipe.
